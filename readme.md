[Ejemplos de Queries con MongoDB](https://geekflare.com/es/mongodb-queries-examples/)

### Comandos Mongo

```
> mongoimport --db tweet --collection final --file final.json
> show databases
> use dolar_dump
> show collections
> db.
```

[Ejemplo de uso de Python](https://tutorial.recursospython.com/?)

TP1
['From Data Mining to Knowledge Discovery in Databases'](https://ojs.aaai.org/index.php/aimagazine/article/view/1230/1131)

### Resumen:

- La minería de datos y el descubrimiento de conocimiento en las bases de datos han atraído una cantidad significativa de atención de la investigación, la industria y los medios de comunicación en los últimos tiempos. ¿De qué se trata toda la emoción? Este artículo proporciona una descripción general de este campo emergente y aclara cómo la extracción de datos y el descubrimiento de conocimiento en las bases de datos se relacionan entre sí y con campos relacionados, como el aprendizaje automático, las estadísticas y las bases de datos. El artículo menciona aplicaciones particulares del mundo real, técnicas específicas de minería de datos, desafíos involucrados en aplicaciones del mundo real del descubrimiento de conocimiento y direcciones de investigación actuales y futuras en el campo.

- en una amplia variedad de campos, los datos se recopilan y acumulan a un ritmo espectacular. Existe una necesidad urgente de una nueva generación de teorías y herramientas computacionales para ayudar a los humanos a extraer información útil (conocimiento) de los volúmenes de datos digitales que crecen rápidamente. Estas teorías y herramientas son el tema del campo emergente del descubrimiento de conocimiento en bases de datos (KDD).

En un nivel abstracto, el campo KDD se ocupa del desarrollo de métodos y técnicas para dar sentido a los datos. El problema básico que aborda el proceso KDD es el mapeo de datos de bajo nivel (que suelen ser demasiado voluminosos para entenderlos y digerirlos fácilmente) en otras formas que pueden ser más compactas (por ejemplo, un informe breve), más abstractas (por ejemplo, , una aproximación descriptiva o modelo del proceso que generó los datos), o más útil (por ejemplo, un modelo predictivo para estimar el valor de casos futuros). En el centro del proceso se encuentra la aplicación de métodos específicos de minería de datos para el descubrimiento y la extracción de patrones.1

Este artículo comienza discutiendo el contexto histórico de KDD y la minería de datos y su intersección con otros campos relacionados. Se proporciona un breve resumen de las aplicaciones recientes de KDD en el mundo real. Se proporcionan definiciones de KDD y minería de datos, y se describe el proceso general de KDD de varios pasos. Este proceso de varios pasos tiene la aplicación de algoritmos de minería de datos como un paso particular en el proceso. El paso de minería de datos se analiza con más detalle en el contexto de algoritmos de minería de datos específicos y su aplicación. También se describen problemas de aplicación práctica en el mundo real. Finalmente, el artículo enumera los desafíos para la investigación y el desarrollo futuros y, en particular, analiza las oportunidades potenciales para la tecnología de IA en los sistemas KDD.

### ¿Por qué necesitamos KDD?

El método tradicional de convertir datos en conocimiento se basa en el análisis y la interpretación manuales. Por ejemplo, en la industria del cuidado de la salud, es común que los especialistas analicen periódicamente las tendencias actuales y los cambios en los datos del cuidado de la salud, por ejemplo, trimestralmente. Luego, los especialistas proporcionan un informe que detalla el análisis a la organización de atención médica patrocinadora; este informe se convierte en la base para la futura toma de decisiones y la planificación de la gestión sanitaria. En un tipo de aplicación totalmente diferente, los geólogos planetarios filtran imágenes de planetas y asteroides obtenidas mediante sensores remotos, localizando y catalogando cuidadosamente objetos geológicos de interés como cráteres de impacto. Ya sea ciencia, marketing, finanzas, atención médica, comercio minorista o cualquier otro campo, el enfoque clásico del análisis de datos se basa fundamentalmente en que uno o más analistas se familiaricen íntimamente con los datos y sirvan como interfaz entre los datos y los usuarios y productos.

Para estas (y muchas otras) aplicaciones, esta forma de sondeo manual de un conjunto de datos es lenta, costosa y muy subjetiva. De hecho, a medida que los volúmenes de datos crecen de forma espectacular, este tipo de análisis de datos manual se está volviendo completamente poco práctico en muchos dominios. Las bases de datos aumentan de tamaño de dos maneras: (1) el número N de registros u objetos en la base de datos y (2) el número d de campos o atributos de un objeto. Las bases de datos que contienen del orden de N = 109 objetos son cada vez más comunes, por ejemplo, en las ciencias astronómicas. De manera similar, el número de campos d puede ser fácilmente del orden de 102 o incluso 103, por ejemplo, en aplicaciones de diagnóstico médico. ¿Quién puede esperar que digiera millones de registros, cada uno con decenas o cientos de campos? Creemos que este trabajo ciertamente no es para humanos; por lo tanto, el trabajo de análisis debe automatizarse, al menos parcialmente.

La necesidad de ampliar las capacidades de análisis humano para manejar la gran cantidad de bytes que podemos recopilar es tanto económica como científica. Las empresas utilizan los datos para obtener una ventaja competitiva, aumentar la eficiencia y brindar servicios más valiosos a los clientes. Los datos que capturamos sobre nuestro entorno son la evidencia básica que usamos para construir teorías y modelos del universo en el que vivimos. Debido a que las computadoras han permitido a los humanos recopilar más datos de los que podemos digerir, es natural recurrir a técnicas computacionales para ayudarnos. desenterrar patrones y estructuras significativos de los volúmenes masivos de datos. Por lo tanto, KDD es un intento de abordar un problema que la era de la información digital convirtió en una realidad para todos nosotros: la sobrecarga de datos.

### Minería de datos y descubrimiento de conocimiento en el mundo real

Gran parte del interés actual en KDD es el resultado del interés de los medios en torno a las aplicaciones exitosas de KDD, por ejemplo, los artículos de enfoque de los últimos dos años en Business Week, Newsweek, Byte, PC Week y otras publicaciones periódicas de gran circulación. Desafortunadamente, no siempre es fácil separar los hechos de la exageración de los medios. No obstante, varios ejemplos bien documentados de sistemas exitosos pueden denominarse correctamente aplicaciones KDD y se han implementado en uso operativo en problemas del mundo real a gran escala en la ciencia y en los negocios.

En ciencia, una de las principales áreas de aplicación es la astronomía. Aquí, SKICAT, un sistema utilizado por los astrónomos para realizar análisis de imágenes, clasificación y catalogación de objetos del cielo a partir de imágenes de estudios del cielo, logró un éxito notable (Fayyad, Djorgovski y Weir 1996). En su primera aplicación, el sistema se usó para procesar los 3 terabytes (1012 bytes) de datos de imágenes resultantes del Segundo Sondeo del Cielo del Observatorio Palomar, donde se estima que son detectables del orden de 109 objetos del cielo. SKICAT puede superar a los humanos y las técnicas computacionales tradicionales en la clasificación de objetos celestes tenues. Véase Fayyad, Haussler y Stolorz (1996) para un estudio de las aplicaciones científicas.

En los negocios, las principales áreas de aplicación de KDD incluyen marketing, finanzas (especialmente inversiones), detección de fraude, fabricación, telecomunicaciones y agentes de Internet. Marketing: en marketing, la aplicación principal son los sistemas de marketing de bases de datos, que analizan las bases de datos de clientes para identificar diferentes grupos de clientes y pronosticar su comportamiento. Business Week (Berry 1994) estimó que más de la mitad de todos los minoristas usan o planean usar marketing de base de datos, y aquellos que lo usan tienen buenos resultados; por ejemplo, American Express informa un aumento del 10 al 15 por ciento en el uso de tarjetas de crédito. Otra aplicación de marketing notable son los sistemas de análisis de canasta de mercado (Agrawal et al. 1996), que encuentran patrones tales como: “Si el cliente compró X, es probable que también compre Y y Z”. Tales patrones son valiosos para los minoristas. Inversión: Numerosas empresas utilizan la minería de datos para la inversión, pero la mayoría no describe sus sistemas. Una excepción es LBS Capital Management. Su sistema utiliza sistemas expertos, redes neuronales y algoritmos genéticos para administrar carteras por un total de $600 millones; desde su inicio en 1993, el sistema ha superado al amplio mercado de valores (Hall, Mani y Barr 1996). Detección de fraude: Los sistemas HNC Falcon y Nestor PRISM se utilizan para monitorear el fraude con tarjetas de crédito, vigilando millones de cuentas. El sistema FAIS (Senator et al. 1995), de la Red de Ejecución de Delitos Financieros del Departamento del Tesoro de EE. UU., se utiliza para identificar transacciones financieras que podrían indicar actividad de lavado de dinero. Fabricación: El sistema de solución de problemas CASSIOPEE, desarrollado como parte de una empresa conjunta entre General Electric y SNECMA, fue aplicado por tres importantes aerolíneas europeas para diagnosticar y predecir problemas en el Boeing 737. Para derivar familias de fallas, se utilizan métodos de ***agrupamiento***. CASSIOPEE recibió el primer premio europeo de aplicaciones innovadoras (Manago y Auriol 1996). Telecomunicaciones: El analizador de secuencias de alarmas de telecomunicaciones (TASA) se construyó en colaboración con un fabricante de equipos de telecomunicaciones y tres redes telefónicas (Mannila, Toivonen y Verkamo 1995). El sistema utiliza un marco novedoso para localizar episodios de alarma que ocurren con frecuencia en el flujo de alarmas y presentarlos como reglas. Se pueden explorar grandes conjuntos de reglas descubiertas con herramientas flexibles de recuperación de información que admiten la interactividad y la iteración. De esta forma, TASA ofrece herramientas de poda, agrupación y ordenamiento para refinar los resultados de una búsqueda básica de reglas por fuerza bruta. Limpieza de datos: Se aplicó el sistema MERGE-PURGE para la identificación de reclamos de asistencia social duplicados (Hernandez y Stolfo 1995). Se utilizó con éxito en datos del Departamento de Bienestar del Estado de Washington. En otras áreas, un sistema muy publicitado es ADVANCED SCOUT de IBM, un sistema especializado de extracción de datos que ayuda a los entrenadores de la Asociación Nacional de Baloncesto (NBA) a organizar e interpretar datos de los juegos de la NBA (U.S. News 1995). ADVANCED SCOUT fue utilizado por varios equipos de la NBA en 1996, incluidos los Seattle Supersonics, que llegaron a la final de la NBA. Finalmente, un tipo de descubrimiento novedoso y cada vez más importante es el que se basa en el uso de agentes inteligentes para navegar a través de un entorno rico en información. Aunque la idea de activadores activos ha sido analizada durante mucho tiempo en el campo de las bases de datos, las aplicaciones realmente exitosas de esta idea aparecieron solo con la llegada de Internet. Estos sistemas solicitan al usuario que especifique un perfil de interés y busque información relacionada entre una amplia variedad de fuentes de dominio público y propiedad. Por ejemplo, FIREFLY es un agente personal de recomendación de música: le pregunta al usuario su opinión sobre varias piezas musicales y luego sugiere otra música que podría gustarle al usuario (<http://www.ffly.com/>). CRAYON (http://crayon.net/>) permite a los usuarios crear su propio periódico gratuito (apoyado por anuncios); NEWSHOUND (<http://www.sjmercury.com/hound/>) de San Jose Mercury News y FARCAST (<http://www.farcast.com/>) buscan automáticamente información de una amplia variedad de fuentes, incluidos periódicos y servicios de cable, y documentos relevantes por correo electrónico directamente al usuario. Estos son solo algunos de los numerosos sistemas de este tipo que utilizan técnicas KDD para producir automáticamente información útil a partir de grandes cantidades de datos sin procesar. Véase Piatetsky-Shapiro et al. (1996). ) para obtener una descripción general de los problemas en el desarrollo de aplicaciones KDD industriales.

### Minería de datos y KDD

Históricamente, la noción de encontrar patrones útiles en los datos ha recibido una variedad de nombres, que incluyen minería de datos, extracción de conocimiento, descubrimiento de información, recolección de información, arqueología de datos y procesamiento de patrones de datos. El término minería de datos ha sido utilizado principalmente por estadísticos, analistas de datos y las comunidades de sistemas de información de gestión (MIS). También ha ganado popularidad en el campo de las bases de datos. La frase descubrimiento de conocimiento en bases de datos se acuñó en el primer taller de KDD en 1989 (Piatetsky-Shapiro 1991) para enfatizar que el conocimiento es el producto final de un descubrimiento basado en datos. Se ha popularizado en los campos de la IA y el aprendizaje automático. Desde nuestro punto de vista, ***KDD se refiere al proceso general de descubrir conocimiento útil a partir de datos, y la minería de datos se refiere a un paso particular en este proceso***. La minería de datos es la aplicación de algoritmos específicos para extraer patrones de los datos. La distinción entre el proceso KDD y el paso de minería de datos (dentro del proceso) es un punto central de este artículo. Los pasos adicionales en el proceso KDD, como la preparación de datos, la selección de datos, la limpieza de datos, la incorporación de conocimientos previos apropiados y la interpretación adecuada de los resultados de la minería, son esenciales para garantizar que se deriven conocimientos útiles de los datos. La aplicación a ciegas de métodos de extracción de datos (correctamente criticados como dragado de datos en la literatura estadística) puede ser una actividad peligrosa, que conduce fácilmente al descubrimiento de patrones inválidos y sin sentido.

### La naturaleza interdisciplinaria de KDD

KDD ha evolucionado y sigue evolucionando a partir de la intersección de campos de investigación como el aprendizaje automático, el reconocimiento de patrones, las bases de datos, las estadísticas, la IA, la adquisición de conocimientos para sistemas expertos, la visualización de datos y la computación de alto rendimiento. El objetivo unificador es extraer conocimiento de alto nivel de datos de bajo nivel en el contexto de grandes conjuntos de datos. El componente de minería de datos de KDD actualmente se basa en gran medida en técnicas conocidas de aprendizaje automático, reconocimiento de patrones y estadísticas para encontrar patrones a partir de datos en el paso de minería de datos del proceso KDD. Una pregunta natural es: ¿En qué se diferencia KDD del reconocimiento de patrones o el aprendizaje automático (y campos relacionados)? La respuesta es que estos campos proporcionan algunos de los métodos de minería de datos que se utilizan en el paso de minería de datos del proceso KDD. KDD se enfoca en el proceso general de descubrimiento de conocimiento a partir de datos, incluido cómo se almacenan y acceden a los datos, cómo se pueden escalar los algoritmos a conjuntos de datos masivos y seguir ejecutándose de manera eficiente, cómo se pueden interpretar y visualizar los resultados, y cómo la interacción hombre-máquina en general puede ser útilmente modelado y soportado. El proceso KDD puede verse como una actividad multidisciplinaria que abarca técnicas que van más allá del alcance de cualquier disciplina en particular, como el aprendizaje automático. En este contexto, existen claras oportunidades para que otros campos de la IA (además del aprendizaje automático) contribuyan a KDD. KDD pone especial énfasis en encontrar patrones comprensibles que puedan interpretarse como conocimiento útil o interesante. Así, por ejemplo, las redes neuronales, aunque son una poderosa herramienta de modelado, son relativamente difíciles de entender en comparación con los árboles de decisión. KDD también enfatiza las propiedades de escalabilidad y robustez de los algoritmos de modelado para grandes conjuntos de datos ruidosos.

Los campos de investigación relacionados con la IA incluyen el descubrimiento de máquinas, que se enfoca en el descubrimiento de leyes empíricas a partir de la observación y la experimentación (Shrager y Langley 1990) (ver Kloesgen y Zytkow [1996] para un glosario de términos comunes a KDD y descubrimiento de máquinas), y modelado causal para la inferencia de modelos causales a partir de datos (Spirtes, Glymour y Scheines 1993). Las estadísticas en particular tienen mucho en común con KDD (ver Elder y Pregibon [1996] y Glymour et al. [1996] para una discusión más detallada de esta sinergia). El descubrimiento de conocimiento a partir de datos es fundamentalmente un esfuerzo estadístico. Las estadísticas proporcionan un lenguaje y un marco para cuantificar la incertidumbre que resulta cuando uno trata de inferir patrones generales de una muestra particular de una población total. Como se mencionó anteriormente, el término minería de datos ha tenido connotaciones negativas en las estadísticas desde la década de 1960, cuando se introdujeron por primera vez las técnicas de análisis de datos basadas en computadora. La preocupación surgió porque si uno busca lo suficiente en cualquier conjunto de datos (incluso datos generados aleatoriamente), puede encontrar patrones que parecen ser estadísticamente significativos pero, de hecho, no lo son. Claramente, este tema es de fundamental importancia para KDD. En los últimos años se han hecho progresos sustanciales en la comprensión de estos temas en las estadísticas. Gran parte de este trabajo es de relevancia directa para KDD. Por lo tanto, la minería de datos es una actividad legítima siempre que se entienda cómo hacerlo correctamente; Debe evitarse la extracción de datos mal realizada (sin tener en cuenta los aspectos estadísticos del problema). También se puede considerar que KDD abarca una visión más amplia del modelado que de las estadísticas. KDD tiene como objetivo proporcionar herramientas para automatizar (en la medida de lo posible) todo el proceso de análisis de datos y el "arte" de la selección de hipótesis del estadístico. Una fuerza impulsora detrás de KDD es el campo de la base de datos (la segunda D en KDD). De hecho, el problema de la manipulación efectiva de datos cuando los datos no caben en la memoria principal es de fundamental importancia para KDD. Las técnicas de bases de datos para obtener un acceso eficiente a los datos, agrupar y ordenar las operaciones al acceder a los datos y optimizar las consultas constituyen los elementos básicos para escalar los algoritmos a conjuntos de datos más grandes. La mayoría de los algoritmos de minería de datos de estadísticas, reconocimiento de patrones y aprendizaje automático asumen que los datos están en la memoria principal y no prestan atención a cómo se descompone el algoritmo si solo son posibles vistas limitadas de los datos. Un campo relacionado que evoluciona a partir de las bases de datos es el almacenamiento de datos, que se refiere a la popular tendencia comercial de recopilar y limpiar datos transaccionales para que estén disponibles para el análisis en línea y el apoyo a la toma de decisiones. El almacenamiento de datos ayuda a preparar el escenario para KDD de dos maneras importantes: (1) limpieza de datos y (2) acceso a datos. Limpieza de datos: a medida que las organizaciones se ven obligadas a pensar en una vista lógica unificada de la amplia variedad de datos y bases de datos que poseen, deben abordar los problemas de asignación de datos a una sola convención de nomenclatura, representación y manejo uniforme de los datos faltantes y manejo del ruido. y errores cuando sea posible. Acceso a los datos: se deben crear métodos uniformes y bien definidos para acceder a los datos y proporcionar rutas de acceso a los datos a los que históricamente era difícil acceder (por ejemplo, almacenados sin conexión). Una vez que las organizaciones y las personas han resuelto el problema de cómo almacenar y acceder a sus datos, el siguiente paso natural es la pregunta: ¿Qué más hacemos con todos los datos? Aquí es donde surgen naturalmente las oportunidades para KDD. Un enfoque popular para el análisis de almacenes de datos se denomina procesamiento analítico en línea (OLAP), llamado así por un conjunto de principios propuestos por Codd (1993). Las herramientas OLAP se enfocan en brindar análisis de datos multidimensionales, que es superior a SQL en el cálculo de resúmenes y desgloses a lo largo de muchas dimensiones. Las herramientas OLAP están destinadas a simplificar y respaldar el análisis de datos interactivo, pero el objetivo de las herramientas KDD es automatizar la mayor parte posible del proceso. Por lo tanto, KDD es un paso más allá de lo que actualmente admiten la mayoría de los sistemas de bases de datos estándar.

### Definiciones basicas

***KDD es el proceso no trivial de identificar patrones válidos, novedosos, potencialmente útiles y, en última instancia, comprensibles en los datos*** (Fayyad, Piatetsky-Shapiro y Smyth 1996). Aquí, los datos son un conjunto de hechos (por ejemplo, casos en una base de datos) y el patrón es una expresión en algún lenguaje que describe un subconjunto de datos o un modelo aplicable al subconjunto. Por lo tanto, en nuestro uso aquí, extraer un patrón también designa ajustar un modelo a los datos; encontrar estructura a partir de datos; o, en general, realizar cualquier descripción de alto nivel de un conjunto de datos. El término proceso implica que KDD comprende muchos pasos, que involucran la preparación de datos, la búsqueda de patrones, la evaluación del conocimiento y el refinamiento, todo repetido en múltiples iteraciones. Por no trivial, queremos decir que está involucrada alguna búsqueda o inferencia; es decir, no es un cálculo sencillo de cantidades predefinidas como calcular el valor promedio de un conjunto de números. Los patrones descubiertos deberían ser válidos en datos nuevos con cierto grado de certeza. También queremos que los patrones sean novedosos (al menos para el sistema y preferiblemente para el usuario) y potencialmente útiles, es decir, que generen algún beneficio para el usuario o la tarea. Finalmente, los patrones deben ser comprensibles, si no inmediatamente, después de un procesamiento posterior. La discusión anterior implica que podemos definir medidas cuantitativas para evaluar los patrones extraídos. En muchos casos, es posible definir medidas de certeza (por ejemplo, precisión de predicción estimada en nuevos datos) o utilidad (por ejemplo, ganancia, tal vez en dólares ahorrados debido a mejores predicciones o aceleración en el tiempo de respuesta de un sistema). Nociones como novedad y comprensibilidad son mucho más subjetivas. En ciertos contextos, la comprensibilidad se puede estimar por simplicidad (por ejemplo, la cantidad de bits para describir un patrón). Una noción importante, llamada interés (por ejemplo, ver Silberschatz y Tuzhilin [1995] y Piatetsky-Shapiro y Matheus [1994]), generalmente se toma como una medida general del valor del patrón, combinando validez, novedad, utilidad y simplicidad. Las funciones de interés pueden definirse explícitamente o pueden manifestarse implícitamente a través de un ordenamiento colocado por el sistema KDD sobre los patrones o modelos descubiertos. Dadas estas nociones, podemos considerar que un patrón es conocimiento si supera algún umbral de interés, lo que de ninguna manera es un intento de definir el conocimiento desde el punto de vista filosófico o incluso popular. De hecho, el conocimiento en esta definición está puramente orientado al usuario y es específico del dominio y está determinado por las funciones y los umbrales que elija el usuario. La minería de datos es un paso en el proceso KDD que consiste en aplicar algoritmos de análisis y descubrimiento de datos que, bajo limitaciones aceptables de eficiencia computacional, producen una enumeración particular de patrones (o modelos) sobre los datos. Tenga en cuenta que el espacio de patrones suele ser infinito, y la enumeración de patrones implica alguna forma de búsqueda en este espacio. Las restricciones computacionales prácticas imponen límites severos al subespacio que puede ser explorado por un algoritmo de minería de datos.

El proceso KDD implica el uso de la base de datos junto con cualquier selección, preprocesamiento, submuestreo y transformaciones requeridos de la misma; aplicar métodos de minería de datos (algoritmos) para enumerar patrones a partir de ellos; y evaluar los productos de la minería de datos para identificar el subconjunto de los patrones enumerados considerados conocimiento. El componente de minería de datos del proceso KDD se ocupa de los medios algorítmicos por los cuales los patrones se extraen y enumeran de los datos. El proceso general de KDD (figura 1) incluye la evaluación y la posible interpretación de los patrones extraídos para determinar qué patrones pueden considerarse nuevos conocimientos. El proceso KDD también incluye todos los pasos adicionales que se describen en la siguiente sección. La noción de un proceso general dirigido por el usuario no es exclusivo de KDD: se han presentado propuestas análogas tanto en estadística (Hand 1994) como en aprendizaje automático (Brodley y Smyth 1996).

### El proceso KDD

El proceso KDD es interactivo e iterativo, involucra numerosos pasos con muchas decisiones tomadas por el usuario. Brachman y Anand (1996) dan una visión práctica del proceso KDD, enfatizando la naturaleza interactiva del proceso. Aquí, describimos en términos generales algunos de sus pasos básicos: Primero, desarrollar una comprensión del dominio de la aplicación y el conocimiento previo relevante e identificar el objetivo del proceso KDD desde el punto de vista del cliente. El segundo es crear un conjunto de datos de destino: seleccionar un conjunto de datos o centrarse en un subconjunto de variables o muestras de datos, en el que se realizará el descubrimiento. El tercero es la limpieza y el preprocesamiento de datos. Las operaciones básicas incluyen eliminar el ruido si corresponde, recopilar la información necesaria para modelar o dar cuenta del ruido, decidir sobre estrategias para manejar los campos de datos faltantes y dar cuenta de la información de secuencia de tiempo y los cambios conocidos. El cuarto es la reducción y proyección de datos: encontrar características útiles para representar los datos según el objetivo de la tarea. Con los métodos de transformación o reducción de dimensionalidad, se puede reducir el número efectivo de variables bajo consideración, o se pueden encontrar representaciones invariantes para los datos. El quinto es hacer coincidir los objetivos del proceso KDD (paso 1) con un método de minería de datos en particular. Por ejemplo, el resumen, la clasificación, la regresión, el agrupamiento, etc., se describen más adelante, así como en Fayyad, Piatetsky-Shapiro y Smyth (1996). El sexto es el análisis exploratorio y la selección de modelos e hipótesis: elegir los algoritmos de extracción de datos y seleccionar los métodos que se utilizarán para buscar patrones de datos. Este proceso incluye decidir qué modelos y parámetros pueden ser apropiados (por ejemplo, los modelos de datos categóricos son diferentes a los modelos de vectores sobre los reales) y hacer coincidir un método de minería de datos particular con los criterios generales del proceso KDD (por ejemplo, el el usuario final podría estar más interesado en comprender el modelo que en sus capacidades predictivas). El séptimo es la minería de datos: la búsqueda de patrones de interés en una forma de representación particular o en un conjunto de tales representaciones, incluidas las reglas de clasificación o los árboles, la regresión y la agrupación. El usuario puede ayudar significativamente al método de minería de datos realizando correctamente los pasos anteriores. El octavo es interpretar patrones extraídos, posiblemente volviendo a cualquiera de los pasos 1 a 7 para una mayor iteración. Este paso también puede implicar la visualización de los patrones y modelos extraídos o la visualización de los datos dados los modelos extraídos. El noveno es actuar sobre el conocimiento descubierto: usar el conocimiento directamente, incorporar el conocimiento en otro sistema para una acción posterior, o simplemente documentarlo e informarlo a las partes interesadas. Este proceso también incluye verificar y resolver posibles conflictos con el conocimiento previamente creído (o extraído). El proceso KDD puede implicar una iteración significativa y puede contener bucles entre dos pasos cualesquiera. El flujo básico de pasos (aunque no la multitud potencial de iteraciones y bucles) se ilustra en la figura 1. La mayoría de los trabajos anteriores sobre KDD se han centrado en el paso 7, la extracción de datos. Sin embargo, los otros pasos son tan importantes (y probablemente más) para la aplicación exitosa de KDD en la práctica. Habiendo definido las nociones básicas e introducido el proceso KDD, ahora nos enfocamos en el componente de minería de datos, que, con mucho, ha recibido la mayor atención en la literatura.

### El paso de minería de datos del proceso KDD

El componente de minería de datos del proceso KDD a menudo implica la aplicación iterativa repetida de métodos particulares de minería de datos. Esta sección presenta una descripción general de los objetivos principales de la minería de datos, una descripción de los métodos utilizados para abordar estos objetivos y una breve descripción de los algoritmos de minería de datos que incorporan estos métodos. Los objetivos de descubrimiento de conocimiento están definidos por el uso previsto del sistema. Podemos distinguir dos tipos de objetivos: (1) verificación y (2) descubrimiento. Con la verificación, el sistema se limita a verificar la hipótesis del usuario. Con el descubrimiento, el sistema encuentra de forma autónoma nuevos patrones. Además, subdividimos el objetivo de descubrimiento en predicción, donde el sistema encuentra patrones para predecir el comportamiento futuro de algunas entidades, y descripción, donde el sistema encuentra patrones para presentarlos a un usuario en una forma comprensible para los humanos. En este artículo, nos ocupamos principalmente de la minería de datos orientada al descubrimiento. La minería de datos implica ajustar modelos o determinar patrones a partir de los datos observados. Los modelos ajustados desempeñan el papel de conocimiento inferido: el hecho de que los modelos reflejen conocimiento útil o interesante es parte del proceso KDD interactivo general en el que normalmente se requiere un juicio humano subjetivo. Se utilizan dos formalismos matemáticos primarios en el ajuste de modelos: (1) estadístico y (2) lógico. El enfoque estadístico permite efectos no deterministas en el modelo, mientras que un modelo lógico es puramente determinista. Nos enfocamos principalmente en el enfoque estadístico de la minería de datos, que tiende a ser la base más utilizada para las aplicaciones prácticas de minería de datos dada la presencia típica de incertidumbre en los procesos de generación de datos del mundo real. La mayoría de los métodos de minería de datos se basan en técnicas probadas de aprendizaje automático, reconocimiento de patrones y estadísticas: clasificación, agrupamiento, regresión, etc. La variedad de algoritmos diferentes bajo cada uno de estos encabezados a menudo puede ser desconcertante tanto para el principiante como para el analista de datos experimentado. Debe enfatizarse que de los muchos métodos de minería de datos anunciados en la literatura, en realidad solo hay unas pocas técnicas fundamentales. La representación real del modelo subyacente que utiliza un método en particular generalmente proviene de una composición de un pequeño número de opciones bien conocidas: polinomios, splines, funciones kernel y base, funciones booleanas de umbral, etc. Por lo tanto, los algoritmos tienden a diferir principalmente en el criterio de bondad de ajuste utilizado para evaluar el ajuste del modelo o en el método de búsqueda utilizado para encontrar un buen ajuste. En nuestra breve descripción general de los métodos de minería de datos, intentamos en particular transmitir la noción de que la mayoría (si no todos) los métodos pueden verse como extensiones o híbridos de algunas técnicas y principios básicos. Primero discutimos los métodos principales de minería de datos y luego mostramos que los métodos de minería de datos pueden verse como compuestos de tres componentes algorítmicos principales: (1) representación del modelo, (2) evaluación del modelo y (3) búsqueda. En la discusión de KDD y los métodos de minería de datos, usamos un ejemplo simple para hacer que algunas de las nociones sean más concretas. La figura 2 muestra un conjunto de datos artificiales bidimensionales simples que consta de 23 casos. Cada punto del gráfico representa a una persona a la que un banco en particular le ha otorgado un préstamo en algún momento del pasado. El eje horizontal representa los ingresos de la persona; el eje vertical representa la deuda personal total de la persona (hipoteca, cuotas del automóvil, etc.). Los datos se han clasificado en dos clases: (1) las x representan a personas que han incumplido con sus préstamos y (2) las o representan a personas cuyos préstamos están en buen estado con el banco. Por lo tanto, este simple conjunto de datos artificiales podría representar un conjunto de datos históricos que pueden contener conocimiento útil desde el punto de vista del banco que otorga los préstamos. Tenga en cuenta que en las aplicaciones KDD reales, normalmente hay muchas más dimensiones (hasta varios cientos) y muchos más puntos de datos (muchos miles o incluso millones).
El propósito aquí es ilustrar ideas básicas sobre un pequeño problema en el espacio bidimensional.

### Métodos de minería de datos

Los dos objetivos principales de alto nivel de la minería de datos en la práctica tienden a ser la predicción y la descripción. Como se indicó anteriormente, la predicción implica el uso de algunas variables o campos en la base de datos para predecir valores desconocidos o futuros de otras variables de interés, y la descripción se enfoca en encontrar patrones interpretables por humanos que describen los datos. Aunque los límites entre la predicción y la descripción no son nítidos (algunos de los modelos predictivos pueden ser descriptivos, en la medida en que sean comprensibles, y viceversa), la distinción es útil para comprender el objetivo general del descubrimiento. La importancia relativa de la predicción y la descripción para aplicaciones particulares de minería de datos puede variar considerablemente. Los objetivos de predicción y descripción se pueden lograr utilizando una variedad de métodos particulares de minería de datos. La clasificación es aprender una función que asigna (clasifica) un elemento de datos en una de varias clases predefinidas (Weiss y Kulikowski 1991; Hand 1981). Los ejemplos de métodos de clasificación utilizados como parte de las aplicaciones de descubrimiento de conocimiento incluyen la clasificación de tendencias en los mercados financieros (Apte y Hong 1996) y la identificación automatizada de objetos de interés en bases de datos de imágenes grandes (Fayyad, Djorgovski y Weir 1996). La Figura 3 muestra una partición simple de los datos de préstamo en dos regiones de clase; tenga en cuenta que no es posible separar las clases perfectamente utilizando un límite de decisión lineal. El banco podría querer usar las regiones de clasificación para decidir automáticamente si los futuros solicitantes de préstamos recibirán un préstamo o no. La regresión está aprendiendo una función que asigna un elemento de datos a una variable de predicción de valor real. Las aplicaciones de la regresión son muchas, por ejemplo, predecir la cantidad de biomasa presente en un bosque dadas las mediciones de microondas de sensores remotos, estimar la probabilidad de que un paciente sobreviva dados los resultados de un conjunto de pruebas de diagnóstico, predecir la demanda de los consumidores de un nuevo producto como la función del gasto publicitario y la predicción de series temporales en las que las variables de entrada pueden ser versiones retrasadas en el tiempo de la variable de predicción. La Figura 4 muestra el resultado de una regresión lineal simple donde la deuda total se ajusta como una función lineal de los ingresos: El ajuste es pobre porque solo existe una correlación débil entre las dos variables. El agrupamiento es una tarea descriptiva común en la que se busca identificar un conjunto finito de categorías o grupos para describir los datos (Jain y Dubes 1988; Titterington, Smith y Makov 1985). Las categorías pueden ser mutuamente excluyentes y exhaustivas o consistir en una representación más rica, como categorías jerárquicas o superpuestas. Los ejemplos de aplicaciones de agrupamiento en un contexto de descubrimiento de conocimiento incluyen el descubrimiento de subpoblaciones homogéneas para consumidores en bases de datos de marketing y la identificación de subcategorías de espectros a partir de mediciones del cielo infrarrojo (Cheeseman y Stutz 1996). La Figura 5 muestra una posible agrupación del conjunto de datos de préstamos en tres grupos; tenga en cuenta que los grupos se superponen, lo que permite que los puntos de datos pertenezcan a más de un grupo. Las etiquetas de clase originales (indicadas por x y o en las figuras anteriores) han sido reemplazadas por un + para indicar que ya no se supone que se conoce la pertenencia a la clase. Estrechamente relacionado con el agrupamiento está la tarea de estimación de densidad de probabilidad, que consiste en técnicas para estimar a partir de datos la función de densidad de probabilidad multivariante conjunta de todas las variables o campos en la base de datos (Silverman 1986). El resumen implica métodos para encontrar una descripción compacta para un subconjunto de datos. Un ejemplo simple sería tabular la media y las desviaciones estándar para todos los campos. Los métodos más sofisticados involucran la derivación de reglas de resumen (Agrawal et al. 1996), técnicas de visualización multivariante y el descubrimiento de relaciones funcionales entre variables (Zembowicz y Zytkow 1996). Las técnicas de resumen a menudo se aplican al análisis interactivo de datos exploratorios y la generación automatizada de informes. El modelado de dependencias consiste en encontrar un modelo que describa dependencias significativas entre variables. Los modelos de dependencia existen en dos niveles: (1) el nivel estructural del modelo especifica (a menudo en forma gráfica) qué variables son localmente dependientes entre sí y (2) el nivel cuantitativo del modelo especifica las fortalezas de las dependencias utilizando algunos valores numéricos. escala. Por ejemplo, las redes de dependencia probabilística utilizan la independencia condicional para especificar el aspecto estructural del modelo y las probabilidades o correlaciones para especificar las fortalezas de las dependencias (Glymour et al. 1987; Heckerman 1996). Las redes de dependencia probabilística encuentran cada vez más aplicaciones en áreas tan diversas como el desarrollo de sistemas expertos médicos probabilísticos a partir de bases de datos, recuperación de información y modelado del genoma humano. La detección de cambios y desviaciones se centra en descubrir los cambios más significativos en los datos a partir de valores normativos o medidos previamente (Berndt y Clifford 1996).

### Los componentes de los algoritmos de minería de datos

El siguiente paso es construir algoritmos específicos para implementar los métodos generales que describimos. Se pueden identificar tres componentes principales en cualquier algoritmo de minería de datos: (1) representación del modelo, (2) evaluación del modelo y (3) búsqueda. Esta visión reduccionista no es necesariamente completa ni totalmente abarcadora; más bien, es una forma conveniente de expresar los conceptos clave de los algoritmos de minería de datos de una manera relativamente unificada y compacta. Cheeseman (1990) describe una estructura similar. La representación del modelo es el lenguaje utilizado para describir patrones detectables. Si la representación es demasiado limitada, entonces ninguna cantidad de tiempo de entrenamiento o ejemplos pueden producir un modelo preciso para los datos. Es importante que un analista de datos comprenda completamente los supuestos de representación que pueden ser inherentes a un método en particular. Es igualmente importante que un diseñador de algoritmos establezca claramente qué supuestos de representación están siendo realizados por un algoritmo en particular. Tenga en cuenta que el mayor poder de representación de los modelos aumenta el peligro de sobreajustar los datos de entrenamiento, lo que da como resultado una precisión de predicción reducida en datos no vistos. Los criterios de evaluación del modelo son declaraciones cuantitativas (o funciones de ajuste) de qué tan bien un patrón particular (un modelo y sus parámetros) cumple con los objetivos del proceso KDD. Por ejemplo, los modelos predictivos a menudo se juzgan por la precisión de la predicción empírica en algún conjunto de prueba. Los modelos descriptivos se pueden evaluar a lo largo de las dimensiones de precisión predictiva, novedad, utilidad y comprensibilidad del modelo ajustado. El método de búsqueda consta de dos componentes: (1) búsqueda de parámetros y (2) búsqueda de modelos. Una vez fijados la representación del modelo (o familia de representaciones) y los criterios de evaluación del modelo, el problema de minería de datos se ha reducido a una mera tarea de optimización: encontrar los parámetros y modelos de la familia seleccionada que optimicen los criterios de evaluación. En la búsqueda de parámetros, el algoritmo debe buscar los parámetros que optimizan los criterios de evaluación del modelo dados los datos observados y una representación fija del modelo. La búsqueda de modelos ocurre como un bucle sobre el método de búsqueda de parámetros: la representación del modelo se cambia para que se considere una familia de modelos.

### Algunos métodos de minería de datos

Existe una amplia variedad de métodos de minería de datos, pero aquí solo nos enfocamos en un subconjunto de técnicas populares. Cada método se analiza en el contexto de la representación del modelo, la evaluación del modelo y la búsqueda. 

### Árboles de decisión y reglas

Los árboles de decisión y las reglas que usan divisiones univariadas tienen una forma de representación simple, lo que hace que el modelo inferido sea relativamente fácil de comprender para el usuario. Sin embargo, la restricción a una representación de árbol o regla en particular puede restringir significativamente la forma funcional (y, por lo tanto, el poder de aproximación) del modelo. Por ejemplo, la figura 6 ilustra el efecto de una división de umbral aplicada a la variable de ingresos para un conjunto de datos de préstamo: Está claro que el uso de divisiones de umbral tan simples (paralelas a los ejes de características) limita severamente el tipo de límites de clasificación que se pueden inducir. . Si uno amplía el espacio del modelo para permitir expresiones más generales (como hiperplanos multivariados en ángulos arbitrarios), entonces el modelo es más poderoso para la predicción pero puede ser mucho más difícil de comprender. Un gran número de árboles de decisión y algoritmos de inducción de reglas se describen en la literatura sobre aprendizaje automático y estadística aplicada (Quinlan 1992; Breiman et al. 1984). En gran medida, dependen de métodos de evaluación de modelos basados ​​en la probabilidad, con diversos grados de sofisticación en términos de penalización de la complejidad del modelo. Los métodos de búsqueda codiciosos, que implican reglas de crecimiento y poda y estructuras de árbol, se utilizan normalmente para explorar el espacio superexponencial de los modelos posibles. Los árboles y las reglas se utilizan principalmente para el modelado predictivo, tanto para la clasificación (Apte y Hong 1996; Fayyad, Djorgovski y Weir 1996) como para la regresión, aunque también se pueden aplicar al modelado descriptivo resumido (Agrawal et al. 1996).

### Métodos de clasificación y regresión no lineal

Estos métodos consisten en una familia de técnicas de predicción que ajustan combinaciones lineales y no lineales de funciones básicas (sigmoides, splines, polinomios) a combinaciones de las variables de entrada. Los ejemplos incluyen redes neuronales feedforward, métodos spline adaptativos y regresión de búsqueda de proyección (ver Elder y Pregibon [1996], Cheng y Titterington [1994] y Friedman [1989] para discusiones más detalladas). Considere las redes neuronales, por ejemplo. La Figura 7 ilustra el tipo de límite de decisión no lineal que una red neuronal podría encontrar para el conjunto de datos del préstamo. En términos de evaluación de modelos, aunque las redes del tamaño apropiado pueden aproximarse universalmente a cualquier función suave con cualquier grado de precisión deseado, se sabe relativamente poco sobre las propiedades de representación de las redes de tamaño fijo estimadas a partir de conjuntos de datos finitos. Además, las funciones de error cuadrático estándar y pérdida de entropía cruzada utilizadas para entrenar redes neuronales pueden verse como funciones de probabilidad logarítmica para regresión y clasificación, respectivamente (Ripley 1994; Geman, Bienenstock y Doursat 1992). La propagación hacia atrás es un método de búsqueda de parámetros que realiza un descenso de gradiente en el espacio de parámetros (peso) para encontrar un máximo local de la función de probabilidad a partir de condiciones iniciales aleatorias. Los métodos de regresión no lineal, aunque poderosos en poder de representación, pueden ser difíciles de interpretar. Por ejemplo, aunque los límites de clasificación de la figura 7 pueden ser más precisos que el límite de umbral simple de la figura 6, el límite de umbral tiene la ventaja de que el modelo se puede expresar, con cierto grado de certeza, como una regla simple de la forma " si el ingreso es mayor que el umbral, entonces el préstamo tendrá un buen estado”.

### Métodos basados en ejemplos

La representación es simple: use ejemplos representativos de la base de datos para aproximar un modelo; es decir, las predicciones sobre nuevos ejemplos se derivan de las propiedades de ejemplos similares en el modelo cuya predicción se conoce. Las técnicas incluyen algoritmos de clasificación y regresión del vecino más cercano (Dasarathy 1991) y sistemas de razonamiento basados ​​en casos (Kolodner 1993). La Figura 8 ilustra el uso de un clasificador de vecino más cercano para el conjunto de datos de préstamo: La clase en cualquier punto nuevo en el espacio bidimensional es la misma que la clase del punto más cercano en el conjunto de datos de entrenamiento original. Una desventaja potencial de los métodos basados ​​en ejemplos (en comparación con los métodos basados ​​en árboles) es que se requiere una métrica de distancia bien definida para evaluar la distancia entre puntos de datos. Para los datos de préstamos de la figura 8, esto no sería un problema porque los ingresos y la deuda se miden en las mismas unidades. Sin embargo, si se quisieran incluir variables como la duración del préstamo, el sexo y la profesión, se requeriría más esfuerzo para definir una métrica sensata entre las variables. La evaluación del modelo generalmente se basa en estimaciones de validación cruzada (Weiss y Kulikowski 1991) de un error de predicción: los parámetros del modelo a estimar pueden incluir la cantidad de vecinos que se usarán para la predicción y la métrica de distancia en sí. Al igual que los métodos de regresión no lineal, los métodos basados ​​en ejemplos suelen ser asintóticamente poderosos en términos de propiedades de aproximación pero, por el contrario, pueden ser difíciles de interpretar porque el modelo está implícito en los datos y no está formulado explícitamente. Las técnicas relacionadas incluyen la estimación de la densidad del kernel (Silverman 1986) y el modelado de mezclas (Titterington, Smith y Makov 1985).

### Modelos probabilísticos de dependencia gráfica

Los modelos gráficos especifican dependencias probabilísticas utilizando una estructura gráfica (Whittaker 1990; Pearl 1988). En su forma más simple, el modelo especifica qué variables dependen directamente unas de otras. Por lo general, estos modelos se utilizan con variables categóricas o de valores discretos, pero también son posibles las extensiones a casos especiales, como densidades gaussianas, para variables de valores reales. Dentro de las comunidades estadísticas y de IA, estos modelos se desarrollaron inicialmente en el marco de los sistemas expertos probabilísticos; la estructura del modelo y los parámetros (las probabilidades condicionales adjuntas a los enlaces del gráfico) se obtuvieron de los expertos. Recientemente, ha habido un trabajo significativo tanto en la IA como en las comunidades estadísticas sobre métodos mediante los cuales tanto la estructura como los parámetros de los modelos gráficos pueden aprenderse directamente de las bases de datos (Buntine 1996; Heckerman 1996). Los criterios de evaluación del modelo suelen tener forma bayesiana, y la estimación de parámetros puede ser una combinación de estimaciones de forma cerrada y métodos iterativos, dependiendo de si una variable se observa directamente o se oculta. La búsqueda de modelos puede consistir en métodos codiciosos de escalada en varias estructuras gráficas. El conocimiento previo, como un ordenamiento parcial de las variables basado en relaciones causales, puede ser útil para reducir el espacio de búsqueda del modelo. Aunque aún se encuentran principalmente en la fase de investigación, los métodos de inducción de modelos gráficos son de particular interés para KDD porque la forma gráfica del modelo se presta fácilmente a la interpretación humana.

### Modelos de aprendizaje relacional

Aunque los árboles de decisión y las reglas tienen una representación restringida a la lógica proposicional, el aprendizaje relacional (también conocido como programación lógica inductiva) utiliza el lenguaje de patrones más flexible de la lógica de primer orden. Un estudiante relacional puede encontrar fácilmente fórmulas como X = Y. La mayoría de las investigaciones realizadas hasta la fecha sobre métodos de evaluación de modelos para el aprendizaje relacional son de naturaleza lógica. El poder de representación adicional de los modelos relacionales tiene el precio de demandas computacionales significativas en términos de búsqueda. Ver Dzeroski (1996) para una discusión más detallada.

### Discusión

Dado el amplio espectro de métodos y algoritmos de minería de datos, nuestra descripción general tiene un alcance inevitablemente limitado; muchas técnicas de minería de datos, particularmente métodos especializados para tipos particulares de datos y dominios, no se mencionaron específicamente. Creemos que la discusión general sobre tareas y componentes de minería de datos tiene relevancia general para una variedad de métodos. Por ejemplo, considere la predicción de series temporales, que tradicionalmente se ha presentado como una tarea de regresión predictiva (modelos autorregresivos, etc.). Recientemente, se han desarrollado modelos más generales para aplicaciones de series de tiempo, como funciones de base no lineales, modelos basados ​​en ejemplos y métodos kernel. Además, ha habido un interés significativo en el modelado de datos locales y gráficos descriptivos de series de tiempo en lugar del modelado puramente predictivo (Weigend y Gershenfeld 1993). Por lo tanto, aunque diferentes algoritmos y aplicaciones pueden parecer diferentes en la superficie, no es raro encontrar que comparten muchos componentes comunes. Comprender la minería de datos y la inducción de modelos en este nivel de componente aclara el comportamiento de cualquier algoritmo de minería de datos y facilita que el usuario comprenda su contribución general y su aplicabilidad al proceso KDD. Un punto importante es que cada técnica generalmente se adapta mejor a algunos problemas que a otros. Por ejemplo, los clasificadores de árbol de decisión pueden ser útiles para encontrar estructura en espacios de alta dimensión y en problemas con datos continuos y categóricos mixtos (porque los métodos de árbol no requieren métricas de distancia). Sin embargo, los árboles de clasificación pueden no ser adecuados para problemas en los que los verdaderos límites de decisión entre clases se describen mediante un polinomio de segundo orden (por ejemplo). Por lo tanto, no existe un método universal de minería de datos, y elegir un algoritmo particular para una aplicación particular es algo así como un arte. En la práctica, una gran parte del esfuerzo de la aplicación puede dedicarse a formular correctamente el problema (hacer la pregunta correcta) en lugar de optimizar los detalles algorítmicos de un método de minería de datos en particular (Langley y Simon 1995; Hand 1994). Debido a que nuestra discusión y descripción general de los métodos de minería de datos ha sido breve, queremos dejar en claro dos puntos importantes: primero, nuestra descripción general de la búsqueda automatizada se centró principalmente en los métodos automatizados para extraer patrones o modelos de los datos. Aunque este enfoque es consistente con la definición que dimos anteriormente, no representa necesariamente lo que otras comunidades podrían denominar minería de datos. Por ejemplo, algunos usan el término para designar cualquier búsqueda manual de los datos o búsqueda asistida por consultas a un sistema de gestión de bases de datos o para referirse a humanos que visualizan patrones en los datos. En otras comunidades, se utiliza para referirse a la correlación automatizada de datos de transacciones o la generación automatizada de informes de transacciones. Elegimos centrarnos solo en métodos que contienen ciertos grados de autonomía de búsqueda.

En segundo lugar, tenga cuidado con las exageraciones: el estado del arte en los métodos automatizados de minería de datos aún se encuentra en una etapa bastante temprana de desarrollo. No existen criterios establecidos para decidir qué métodos usar en qué circunstancias, y muchos de los enfoques se basan en aproximaciones heurísticas crudas para evitar la costosa búsqueda necesaria para encontrar soluciones óptimas o incluso buenas. Por lo tanto, el lector debe tener cuidado cuando se enfrente a afirmaciones exageradas sobre la gran capacidad de un sistema para extraer información útil de bases de datos grandes (o incluso pequeñas).

### Problemas de la aplicación

Para ver un estudio de las aplicaciones de KDD, así como ejemplos detallados, consulte Piatetsky-Shapiro et al. (1996) para aplicaciones industriales y Fayyad, Haussler y Stolorz (1996) para aplicaciones en análisis de datos científicos. Aquí, examinamos los criterios para seleccionar aplicaciones potenciales, que se pueden dividir en categorías prácticas y técnicas. Los criterios prácticos para los proyectos KDD son similares a los de otras aplicaciones de tecnología avanzada e incluyen el impacto potencial de una aplicación, la ausencia de soluciones alternativas más simples y un fuerte apoyo organizacional para usar la tecnología. Para las aplicaciones que manejan datos personales, también se deben considerar las cuestiones legales y de privacidad (Piatetsky-Shapiro 1995). Los criterios técnicos incluyen consideraciones como la disponibilidad de datos suficientes (casos). En general, cuantos más campos haya y más complejos sean los patrones que se buscan, más datos se necesitan. Sin embargo, un sólido conocimiento previo (ver discusión más adelante) puede reducir significativamente el número de casos necesarios. Otra consideración es la relevancia de los atributos. Es importante tener atributos de datos que sean relevantes para la tarea de descubrimiento; ninguna cantidad de datos permitirá la predicción basada en atributos que no capturen la información requerida. Además, los bajos niveles de ruido (pocos errores de datos) son otra consideración. Grandes cantidades de ruido dificultan la identificación de patrones a menos que una gran cantidad de casos pueda mitigar el ruido aleatorio y ayudar a aclarar los patrones agregados. Los datos cambiantes y orientados al tiempo, aunque dificultan el desarrollo de la aplicación, lo hacen potencialmente mucho más útil porque es más fácil volver a entrenar un sistema que un ser humano. **Finalmente, y quizás una de las consideraciones más importantes, es el conocimiento previo**. ***Es útil saber algo sobre el dominio: cuáles son los campos importantes, cuáles son las relaciones probables, cuál es la función de utilidad del usuario, qué patrones ya se conocen, etc.***

### Desafíos de investigación y aplicación

Describimos algunos de los principales desafíos actuales de investigación y aplicación para KDD. Esta lista no es exhaustiva y pretende dar al lector una idea de los tipos de problemas con los que luchan los profesionales de KDD. **Bases de datos más grandes**: las bases de datos con cientos de campos y tablas y millones de registros y de un tamaño de varios gigabytes son comunes, y las bases de datos de terabytes (1012 bytes) están comenzando a aparecer. Los métodos para manejar grandes volúmenes de datos incluyen algoritmos más eficientes (Agrawal et al. 1996), muestreo, aproximación y procesamiento masivo en paralelo (Holsheimer et al. 1996). **Alta dimensionalidad**: No solo suele haber una gran cantidad de registros en la base de datos, sino que también puede haber una gran cantidad de campos (atributos, variables); por lo tanto, la dimensionalidad del problema es alta. Un conjunto de datos de alta dimensión crea problemas en términos de aumentar el tamaño del espacio de búsqueda para la inducción de modelos de una manera combinatoriamente explosiva. Además, aumenta las posibilidades de que un algoritmo de minería de datos encuentre patrones falsos que no son válidos en general. Los enfoques de este problema incluyen métodos para reducir la dimensionalidad efectiva del problema y el uso de conocimientos previos para identificar variables irrelevantes. **Sobreajuste**: cuando el algoritmo busca los mejores parámetros para un modelo en particular usando un conjunto limitado de datos, puede modelar no solo los patrones generales en los datos sino también cualquier ruido específico del conjunto de datos, lo que resulta en un rendimiento deficiente del modelo en datos de prueba. Las posibles soluciones incluyen validación cruzada, regularización y otras estrategias estadísticas sofisticadas. **Evaluación de la significación estadística**: se produce un problema (relacionado con el sobreajuste) cuando el sistema busca entre muchos modelos posibles. Por ejemplo, si un sistema prueba modelos con un nivel de significación de 0,001, entonces, en promedio, con datos puramente aleatorios, N/1000 de estos modelos se aceptarán como significativos.

Con frecuencia, muchos intentos iniciales de KDD pasan por alto este punto. Una forma de lidiar con este problema es utilizar métodos que ajusten el estadístico de prueba en función de la búsqueda, por ejemplo, ajustes de Bonferroni para pruebas independientes o pruebas de aleatorización. Datos y conocimientos cambiantes: los datos que cambian rápidamente (no estacionarios) pueden hacer que los patrones descubiertos previamente no sean válidos. Además, las variables medidas en una base de datos de aplicación determinada se pueden modificar, eliminar o aumentar con nuevas medidas a lo largo del tiempo. Las posibles soluciones incluyen métodos incrementales para actualizar los patrones y tratar el cambio como una oportunidad de descubrimiento usándolo para indicar la búsqueda de patrones de cambio únicamente (Matheus, Piatetsky-Shapiro y McNeill 1996). Véase también Agrawal y Psaila (1995) y Mannila, Toivonen y Verkamo (1995). Datos faltantes y ruidosos: este problema es especialmente agudo en las bases de datos comerciales. Según se informa, los datos del censo de EE. UU. tienen tasas de error de hasta el 20 por ciento en algunos campos. Pueden faltar atributos importantes si la base de datos no se diseñó teniendo en cuenta el descubrimiento. Las posibles soluciones incluyen estrategias estadísticas más sofisticadas para identificar variables ocultas y dependencias (Heckerman 1996; Smyth et al. 1996). Relaciones complejas entre campos: los atributos o valores estructurados jerárquicamente, las relaciones entre atributos y los medios más sofisticados para representar el conocimiento sobre los contenidos de una base de datos requerirán algoritmos que puedan usar dicha información de manera efectiva. Históricamente, los algoritmos de minería de datos se han desarrollado para registros de valores de atributos simples, aunque se están desarrollando nuevas técnicas para derivar relaciones entre variables (Dzeroski 1996; Djoko, Cook y Holder 1995). Comprensibilidad de los patrones: en muchas aplicaciones, es importante hacer que los descubrimientos sean más comprensibles para los humanos. Las posibles soluciones incluyen representaciones gráficas (Buntine 1996; Heckerman 1996), estructuración de reglas, generación de lenguaje natural y técnicas para la visualización de datos y conocimientos. Las estrategias de refinamiento de reglas (por ejemplo, Major y Mangano [1995]) se pueden utilizar para abordar un problema relacionado: el conocimiento descubierto puede ser implícita o explícitamente redundante. Interacción del usuario y conocimiento previo: muchos métodos y herramientas actuales de KDD no son realmente interactivos y no pueden incorporar fácilmente el conocimiento previo sobre un problema, excepto de manera simple. El uso del conocimiento del dominio es importante en todos los pasos del proceso KDD. Los enfoques bayesianos (por ejemplo, Cheeseman [1990]) utilizan probabilidades previas sobre datos y distribuciones como una forma de codificar el conocimiento previo. Otros emplean capacidades de base de datos deductivas para descubrir conocimientos que luego se utilizan para guiar la búsqueda de minería de datos (por ejemplo, Simoudis, Livezey y Kerber [1995]). Integración con otros sistemas: un sistema de descubrimiento independiente podría no ser muy útil. Los problemas típicos de integración incluyen la integración con un sistema de administración de base de datos (por ejemplo, a través de una interfaz de consulta), la integración con hojas de cálculo y herramientas de visualización, y la adaptación de lecturas de sensores en tiempo real. Simoudis, Livezey y Kerber (1995) y Stolorz, Nakamura, Mesrobiam, Muntz, Shek, Santos, Yi, Ng, Chien, Mechoso y Farrara (1995) describen ejemplos de sistemas KDD integrados.

### Observaciones finales: el papel potencial de la IA en KDD

Además del aprendizaje automático, otros campos de la IA pueden contribuir significativamente a varios aspectos del proceso KDD. Mencionamos algunos ejemplos de estas áreas aquí: El lenguaje natural presenta oportunidades significativas para la minería en texto de formato libre, especialmente para la anotación e indexación automatizadas antes de la clasificación de corpus de texto. Las capacidades de análisis limitadas pueden ayudar sustancialmente en la tarea de decidir a qué se refiere un artículo. Por lo tanto, el espectro desde el procesamiento simple del lenguaje natural hasta la comprensión del lenguaje puede ser de gran ayuda. Además, el procesamiento del lenguaje natural puede contribuir significativamente como una interfaz efectiva para establecer sugerencias para extraer algoritmos y visualizar y explicar el conocimiento derivado de un sistema KDD. La planificación considera un complicado proceso de análisis de datos. Implica realizar operaciones complicadas de acceso y transformación de datos; aplicar rutinas de preprocesamiento; y, en algunos casos, prestar atención a las limitaciones de recursos y acceso a datos. Por lo general, los pasos de procesamiento de datos se expresan en términos de condiciones previas y posteriores deseadas para la aplicación de ciertas rutinas, lo que se presta fácilmente a la representación como un problema de planificación. Además, la capacidad de planificación puede desempeñar un papel importante en los agentes automatizados (consulte el siguiente elemento) para recopilar muestras de datos o realizar una búsqueda para obtener los conjuntos de datos necesarios. Los agentes inteligentes pueden activarse para recopilar la información necesaria de una variedad de fuentes. Además, los agentes de información pueden activarse de forma remota a través de la red o pueden activarse ante la ocurrencia de un determinado evento e iniciar una operación de análisis. Finalmente, los agentes pueden ayudar a navegar y modelar la World-Wide Web (Etzioni 1996), otra área que crece en importancia. La incertidumbre en la IA incluye problemas para gestionar la incertidumbre, los mecanismos de inferencia adecuados en presencia de incertidumbre y el razonamiento sobre la causalidad, todos fundamentales para la teoría y la práctica de KDD. De hecho, la conferencia KDD-96 tuvo una sesión conjunta con la conferencia UAI-96 este año (Horvitz y Jensen 1996). La representación del conocimiento incluye ontologías, nuevos conceptos para representar, almacenar y acceder al conocimiento. También se incluyen esquemas para representar el conocimiento y permitir el uso del conocimiento humano previo sobre el proceso subyacente por parte del sistema KDD. Estas posibles contribuciones de la IA no son más que una muestra; muchos otros, incluida la interacción humano-computadora, las técnicas de adquisición de conocimientos y el estudio de los mecanismos de razonamiento, tienen la oportunidad de contribuir a KDD. En conclusión, presentamos algunas definiciones de nociones básicas en el campo KDD. Nuestro objetivo principal era aclarar la relación entre el descubrimiento de conocimiento y la minería de datos. Brindamos una descripción general del proceso KDD y los métodos básicos de extracción de datos. Dado el amplio espectro de métodos y algoritmos de minería de datos, nuestra descripción general tiene un alcance inevitablemente limitado: existen muchas técnicas de minería de datos, en particular métodos especializados para tipos particulares de datos y dominios. Aunque varios algoritmos y aplicaciones pueden parecer bastante diferentes en la superficie, no es raro encontrar que comparten muchos componentes comunes. Comprender la minería de datos y la inducción de modelos en este nivel de componente aclara la tarea de cualquier algoritmo de minería de datos y facilita que el usuario comprenda su contribución general y su aplicabilidad al proceso KDD. Este artículo representa un paso hacia un marco común que esperamos finalmente proporcione una visión unificadora de los objetivos y métodos generales comunes utilizados en KDD. Esperamos que esto eventualmente conduzca a una mejor comprensión de la variedad de enfoques en este campo multidisciplinario y cómo encajan entre sí.

### Usama Fayyad 
Es investigador sénior en Microsoft Research. Recibió su Ph.D. en 1991 de la Universidad de Michigan en Ann Arbor. Antes de unirse a Microsoft en 1996, dirigió el Grupo de Sistemas de Aprendizaje Automático en el Laboratorio de Propulsión a Chorro (JPL) del Instituto de Tecnología de California, donde desarrolló sistemas de extracción de datos para el análisis automatizado de datos científicos. Permanece afiliado al JPL como científico visitante distinguido. Fayyad recibió el Premio JPL 1993 Lew Allen a la Excelencia en Investigación y la Medalla de Logro Excepcional de la Administración Nacional de Aeronáutica y del Espacio de 1994. Sus intereses de investigación incluyen el descubrimiento de conocimiento en grandes bases de datos, minería de datos, teoría y aplicaciones de aprendizaje automático, reconocimiento de patrones estadísticos y agrupamiento. Fue copresidente del programa de KDD-94 y KDD-95 (la Primera Conferencia Internacional sobre Descubrimiento de Conocimiento y Minería de Datos). Es presidente general de KDD-96, editor en jefe de la revista Data Mining and Knowledge Discovery y coeditor del libro de prensa AAAI de 1996 Advances in Knowledge Discovery and Data Mining.
